#cleaner.py
import os
import sys
import signal
from huggingface_hub import hf_hub_download
# from llama_cpp import Llama
import torch
from transformers import MT5ForConditionalGeneration, MT5Tokenizer

tokenizer = MT5Tokenizer.from_pretrained("google/mt5-small")
model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small") 

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def clean_text(text: str, mode: str = "transcription") -> str:
    print(f"Start Clean Text: text: {text}\nMode:{mode}")
    global cleaned
    if mode == "transcription":
        if not text.strip():
            return ""

        prefix = "Please fix grammar mistakes while keeping the meaning the same: "
        input_text = prefix + text

        inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=128)
            print(f"Outputs: {outputs}")

        cleaned = tokenizer.decode(outputs[0], skip_special_tokens=True)
    elif mode == "translation":
        if not text.strip():
            return ""

        prefix = "Please fix grammar mistakes in the same language while keeping the meaning the same: "
        input_text = prefix + text

        inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=128)
            print(f"Outputs: {outputs}")

        cleaned = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
    print(f"Cleaned Text:{cleaned}")
    cleaned = cleaned.strip()
    return cleaned

# ‚îÄ‚îÄ‚îÄ Graceful Shutdown Handler ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def shutdown_handler():
    print("\nüõë Caught shutdown signal. Cleaning up...")
    sys.exit(0)

signal.signal(signal.SIGINT, shutdown_handler)
signal.signal(signal.SIGTERM, shutdown_handler)

# # ‚îÄ‚îÄ‚îÄ Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# LOCAL_MODEL_PATH = "./models/phi-2.Q4_K_M.gguf"
# CACHE_DIR = "./hf_cache"

# # ‚îÄ‚îÄ‚îÄ Ensure Model is Available ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def ensure_gguf_model(local_path=LOCAL_MODEL_PATH):
#     if not os.path.exists(local_path):
#         print("üì• Downloading Phi-2 quantized model from Hugging Face...")
#         hf_hub_download(
#             repo_id="TheBloke/phi-2-GGUF",
#             filename="phi-2.Q4_K_M.gguf",
#             cache_dir=CACHE_DIR,
#             local_dir="./models",
#             local_dir_use_symlinks=False
#         )
#     else:
#         print("‚úÖ Phi-2 GGUF model already available locally.")

# ensure_gguf_model()

# ‚îÄ‚îÄ‚îÄ Load Llama Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# print("üì¶ Loading Phi-2 quantized model into llama_cpp...")

# llm = Llama(
#     model_path=LOCAL_MODEL_PATH,
#     n_ctx=2048,
#     n_threads=8,  # Tune based on your CPU cores
#     n_batch=64
# )

# ‚îÄ‚îÄ‚îÄ Prewarm Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def prewarm_model():
#     _ = llm("Hello!", max_tokens=5)
#     print("üî• Model prewarmed and ready!")

# prewarm_model()

# ‚îÄ‚îÄ‚îÄ Cleaning Function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def clean_text(text: str, mode: str = "transcription") -> str:
#     if not text:
#         return ""

#     if mode == "transcription":
#         prompt = f"""Fix obvious errors without changing the meaning. 
# Respond only with the corrected transcription text. 

# Original: {text}
# Corrected:"""
#     else:  # mode == "translation"
#         prompt = f"""Fix minor grammar or phrasing mistakes without changing meaning.
# Respond only with the corrected translation text.

# Original: {text}
# Corrected:"""

#     try:
#         print(f"üßπ Cleaning text: {text}")
        
#         response = llm(prompt, max_tokens=128, stop=["Corrected:", "\n\n"])
#         output = response['choices'][0]['text'].strip()

#         # Optionally postprocess to really slice out any duplicate prompt echoes
#         if "Corrected:" in output:
#             output = output.split("Corrected:")[-1].strip()

#         print(f"‚úÖ Cleaned text: {output}")
#         return output

#     except Exception as e:
#         print(f"‚ö†Ô∏è Cleaning failed: {e}")
#         return text  # Fallback